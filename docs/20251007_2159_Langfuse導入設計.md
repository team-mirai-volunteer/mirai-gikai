# Langfuse導入設計

## 概要

議案チャットエンドポイント（`web/src/app/api/chat/route.ts`）にLangfuseを導入し、プロンプト管理とテレメトリー機能を実装する。

## 目的

- プロンプトのバージョン管理をLangfuse管理画面で実施
- Vercel AI SDKの `experimental_telemetry` を通じたトレーシング
- 開発/本番環境の統一的な管理とラベリング

## ブランチゴール

現在のシステムプロンプト（`route.ts`の87-107行目）と難易度指示（`getDifficultyInstructions`）をLangfuseの2つのプロンプト（normal/hard）で置き換え、Vercel AI SDKに統合する実装を完了する。

## アプローチ

**最小構成**: SpanProcessorやinstrumentation.tsは使わず、Langfuseクライアントで直接プロンプトを取得し、Vercel AI SDKの `experimental_telemetry.metadata.langfusePrompt` に渡す方式を採用。

## アーキテクチャ設計

### ディレクトリ構造

```
web/src/lib/llm/
├── langfuse/
│   ├── client.ts              # Langfuseクライアント初期化
│   └── types.ts               # 型定義
├── prompt/
│   ├── repository.ts          # Prompt取得のInterface定義
│   └── langfuse-repository.ts # Langfuse実装
└── index.ts                   # 公開API
```

### レイヤー構造

```
┌─────────────────────────────────────┐
│   API Route (/api/chat/route.ts)   │
└──────────────┬──────────────────────┘
               │
┌──────────────▼──────────────────────┐
│   Prompt Repository Interface       │
│   - getPrompt(name, variables?)     │
└──────────────┬──────────────────────┘
               │
        ┌──────▼──────┐
        │  Langfuse   │
        │ Repository  │
        └──────┬──────┘
               │
┌──────────────▼──────────────────────┐
│   Langfuse Client + OpenTelemetry   │
└─────────────────────────────────────┘
```

## インターフェース設計

### 1. 型定義

```typescript
// web/src/lib/llm/langfuse/types.ts

/**
 * Langfuseプロンプトのメタデータ
 * Vercel AI SDKのexperimental_telemetry.metadata.langfusePromptに渡す
 */
export interface LangfusePromptMetadata {
  name: string;
  version: number;
  config: Record<string, unknown>;
  labels: string[];
  tags: string[];
}

/**
 * プロンプトテンプレート変数
 */
export interface PromptVariables {
  [key: string]: string | number | boolean;
}

/**
 * プロンプト取得結果
 */
export interface PromptResult {
  /** コンパイル済みプロンプト文字列 */
  content: string;
  /** Langfuseメタデータ（telemetryに渡す） */
  langfuseMetadata: LangfusePromptMetadata;
}
```

### 2. PromptRepository Interface

```typescript
// web/src/lib/llm/prompt/repository.ts

import type { PromptVariables, PromptResult } from "../langfuse/types";

export interface PromptRepository {
  /**
   * プロンプトを取得する
   * @param name プロンプト名
   * @param variables テンプレート変数
   * @throws Error プロンプト取得失敗時
   */
  getPrompt(
    name: string,
    variables?: PromptVariables
  ): Promise<PromptResult>;
}
```

### 3. Langfuse実装

```typescript
// web/src/lib/llm/prompt/langfuse-repository.ts

import { Langfuse } from "langfuse";
import type { PromptRepository } from "./repository";
import type { PromptVariables, PromptResult } from "../langfuse/types";

export class LangfusePromptRepository implements PromptRepository {
  constructor(private client: Langfuse) {}

  async getPrompt(
    name: string,
    variables?: PromptVariables
  ): Promise<PromptResult> {
    try {
      // Langfuseからプロンプトを取得
      const fetchedPrompt = await this.client.getPrompt(name);

      // 変数があればコンパイル、なければそのまま使用
      const content = variables
        ? fetchedPrompt.compile(variables)
        : fetchedPrompt.prompt;

      // Vercel AI SDKのtelemetryに渡すメタデータを構築
      const langfuseMetadata = {
        name: fetchedPrompt.name,
        version: fetchedPrompt.version,
        config: fetchedPrompt.config,
        labels: fetchedPrompt.labels || [],
        tags: fetchedPrompt.tags || [],
      };

      return {
        content,
        langfuseMetadata,
      };
    } catch (error) {
      throw new Error(
        `Failed to fetch prompt "${name}" from Langfuse: ${error instanceof Error ? error.message : String(error)}`
      );
    }
  }
}
```

### 4. Langfuseクライアント

```typescript
// web/src/lib/llm/langfuse/client.ts

import { Langfuse } from "langfuse";

let langfuseClient: Langfuse | null = null;

export function getLangfuseClient(): Langfuse {
  if (!langfuseClient) {
    const publicKey = process.env.LANGFUSE_PUBLIC_KEY;
    const secretKey = process.env.LANGFUSE_SECRET_KEY;
    const baseUrl = process.env.LANGFUSE_BASE_URL || "https://cloud.langfuse.com";

    if (!publicKey || !secretKey) {
      throw new Error(
        "Langfuse credentials not configured. Set LANGFUSE_PUBLIC_KEY and LANGFUSE_SECRET_KEY"
      );
    }

    langfuseClient = new Langfuse({
      publicKey,
      secretKey,
      baseUrl,
      release: process.env.VERCEL_ENV || "development",
    });
  }

  return langfuseClient;
}
```

### 5. ファクトリー関数

```typescript
// web/src/lib/llm/index.ts

import { getLangfuseClient } from "./langfuse/client";
import { LangfusePromptRepository } from "./prompt/langfuse-repository";
import type { PromptRepository } from "./prompt/repository";

export function createPromptRepository(): PromptRepository {
  try {
    const client = getLangfuseClient();
    return new LangfusePromptRepository(client);
  } catch (error) {
    console.error("Failed to initialize Langfuse client:", error);
    throw error;
  }
}

// 型も公開
export type { PromptRepository } from "./prompt/repository";
export type { PromptVariables, PromptResult, LangfusePromptMetadata } from "./langfuse/types";
```

## API Route統合

### 修正箇所

`web/src/app/api/chat/route.ts` の `POST` 関数を以下のように修正：

```typescript
import { createPromptRepository } from "@/lib/llm";

export async function POST(req: Request) {
  const { messages } = await req.json();

  const billContext = messages[0]?.metadata?.billContext;
  const difficultyLevel = (messages[0]?.metadata?.difficultyLevel || "normal") as DifficultyLevelEnum;

  const promptRepo = createPromptRepository();

  try {
    // 難易度に応じたプロンプト名を決定
    const promptName = `bill-chat-system-${difficultyLevel}`;

    // Langfuseからプロンプト取得
    const promptResult = await promptRepo.getPrompt(promptName, {
      billName: billContext?.name || "",
      billTitle: billContext?.bill_content?.title || "",
      billSummary: billContext?.bill_content?.summary || "",
      billContent: billContext?.bill_content?.content || "",
    });

    // Vercel AI SDKでストリーミング生成
    const result = streamText({
      model: "openai/gpt-4o-mini",
      system: promptResult.content,
      messages: convertToModelMessages(messages),
      experimental_telemetry: {
        isEnabled: true,
        functionId: "bill-chat",
        metadata: {
          // Langfuseプロンプトメタデータをtelemetryに渡す
          langfusePrompt: JSON.stringify(promptResult.langfuseMetadata),
          billId: billContext?.id || "",
          difficultyLevel,
        },
      },
    });

    return result.toUIMessageStreamResponse();
  } catch (error) {
    console.error("Chat API error:", error);
    return new Response(
      JSON.stringify({
        error: "プロンプトの取得に失敗しました",
        details: error instanceof Error ? error.message : String(error)
      }),
      { status: 500, headers: { "Content-Type": "application/json" } }
    );
  }
}
```

### 重要なポイント

1. **難易度別プロンプト**
   - `getDifficultyInstructions()` 関数は削除
   - 代わりにプロンプト名で難易度を指定：
     - `bill-chat-system-normal` - ふつうモード
     - `bill-chat-system-hard` - 難しいモード
   - Langfuse管理画面で各難易度のプロンプトを個別管理

2. **experimental_telemetry の設定**
   - `isEnabled: true` でtelemetryを有効化
   - `functionId` で処理を識別（Langfuseダッシュボードでの検索に使用）
   - `metadata.langfusePrompt` にJSON文字列化したメタデータを渡す

3. **追加のメタデータ**
   - `billId`, `difficultyLevel` などコンテキスト情報も記録
   - Langfuseダッシュボードでフィルタリング・分析に活用

4. **SpanProcessorは不要**
   - この方式では `instrumentation.ts` や `LangfuseSpanProcessor` の設定は不要
   - Vercel AI SDKが自動的にOpenTelemetryトレースを生成
   - `langfusePrompt` メタデータによりLangfuse側で自動的にプロンプトとトレースが紐付けられる

## 環境変数設定

### `.env` に追加

```bash
# Langfuse Configuration
LANGFUSE_PUBLIC_KEY=pk-lf-xxx
LANGFUSE_SECRET_KEY=sk-lf-xxx
LANGFUSE_BASE_URL=https://cloud.langfuse.com

# 環境ラベル（Vercelでは自動設定される）
# VERCEL_ENV=production|preview|development
```

### 環境ラベリング

Vercelでは `VERCEL_ENV` が自動設定されるため、Langfuseの `release` パラメータとして使用：

- `production` - 本番環境
- `preview` - プレビュー環境（PR）
- `development` - ローカル開発

## トレーシングの仕組み

### Vercel AI SDK → Langfuse の連携フロー

```
1. streamText() 実行
   ↓
2. Vercel AI SDKがOpenTelemetryトレースを自動生成
   ↓
3. experimental_telemetry.metadata.langfusePrompt を検出
   ↓
4. Langfuseがトレースとプロンプトを自動で紐付け
   ↓
5. Langfuseダッシュボードに表示
```

### 記録される情報

- **プロンプト情報**: 名前、バージョン、コンテンツ
- **LLM情報**: モデル名、トークン使用量、レイテンシー
- **カスタムメタデータ**: `billId`, `difficultyLevel` など
- **入出力**: ユーザーメッセージとAIレスポンス
- **エラー情報**: 失敗時のスタックトレース

### 環境ラベリング

`VERCEL_ENV` を `release` として使用することで、環境ごとにトレースを分類：

- **production**: 本番環境のトレース
- **preview**: プレビュー環境（PR）のトレース
- **development**: ローカル開発のトレース

Langfuseダッシュボードで環境別にフィルタリング可能。

## Langfuse管理画面でのプロンプト設定

### プロンプト1: bill-chat-system-normal（ふつうモード）

**プロンプト名**: `bill-chat-system-normal`

**プロンプト内容（テンプレート）**:

```
あなたは日本の議案について説明する専門的なアシスタントです。

議案情報：
- 名称: {{billName}}
- タイトル: {{billTitle}}
- 要約: {{billSummary}}
- 詳細: {{billContent}}

回答の難易度：ふつう（中学生レベルの内容）
- 中学生が理解できる程度の語彙と表現を使用してください
- 専門用語は使用してもよいが、必ず説明を併記してください
- 適度に詳しく、かつ分かりやすい説明を心がけてください
- 具体例を交えて説明してください

ルール：
1. この議案に関する質問にのみ回答する
2. 上記の難易度設定に従って説明する
3. 正確で客観的な情報を提供する
4. 政治的に中立な立場を保つ
5. 回答は600文字以下を目安にしつつ、フレンドリーかつサポーティブな口調で行う
6. 回答が難しい場合は、その旨を丁寧に伝える
7. メッセージのおわりは、会話の深堀りをサポートするような文章で締めくくる
8. ただし、毎回質問で終わると、不自然になるので、適宜調整する
```

**変数**:
- `billName`: string
- `billTitle`: string
- `billSummary`: string
- `billContent`: string

**ラベル（推奨）**:
- `feature: chat`
- `difficulty: normal`
- `version: v1`
- `language: ja`

---

### プロンプト2: bill-chat-system-hard（難しいモード）

**プロンプト名**: `bill-chat-system-hard`

**プロンプト内容（テンプレート）**:

```
あなたは日本の議案について説明する専門的なアシスタントです。

議案情報：
- 名称: {{billName}}
- タイトル: {{billTitle}}
- 要約: {{billSummary}}
- 詳細: {{billContent}}

回答の難易度：難しい（専門用語を含む詳細な内容）
- 専門用語を正確に使用し、詳細で網羅的な説明をしてください
- 法律的な背景や制度的な文脈も含めて説明してください
- 複数の観点から議案を分析し、深い考察を提供してください
- 関連する法令や制度についても言及してください

ルール：
1. この議案に関する質問にのみ回答する
2. 上記の難易度設定に従って説明する
3. 正確で客観的な情報を提供する
4. 政治的に中立な立場を保つ
5. 回答は600文字以下を目安にしつつ、フレンドリーかつサポーティブな口調で行う
6. 回答が難しい場合は、その旨を丁寧に伝える
7. メッセージのおわりは、会話の深堀りをサポートするような文章で締めくくる
8. ただし、毎回質問で終わると、不自然になるので、適宜調整する
```

**変数**:
- `billName`: string
- `billTitle`: string
- `billSummary`: string
- `billContent`: string

**ラベル（推奨）**:
- `feature: chat`
- `difficulty: hard`
- `version: v1`
- `language: ja`

## エラーハンドリング

### エラーケース

1. **Langfuse認証失敗**
   - クライアント初期化時にエラー
   - 500エラーを返却

2. **プロンプト取得失敗**
   - ネットワークエラー
   - プロンプト名が存在しない
   - 500エラーを返却

3. **テンプレート変数不足**
   - 警告ログを出力
   - 空文字列で置換して継続

### エラーレスポンス形式

```json
{
  "error": "プロンプトの取得に失敗しました",
  "details": "Failed to fetch prompt \"bill-chat-system\" from Langfuse: Network error"
}
```

## パッケージ依存関係

### 追加パッケージ

```bash
cd web
pnpm add langfuse
```

```json
{
  "dependencies": {
    "langfuse": "^3.x.x"
  }
}
```

**注意**:
- Vercel AI SDKは既にインストール済み（`ai` パッケージ）
- `langfuse-vercel` パッケージは不要（最小構成では使用しない）
- OpenTelemetry関連のパッケージも不要（Vercel AI SDKに含まれる）

## テスト戦略

### 統合テスト（手動）

1. Langfuse管理画面でプロンプト作成
2. ローカル環境で動作確認
3. Langfuse管理画面でプロンプト変更 → 反映されることを確認
4. エラーケース（認証失敗、プロンプト未存在）の確認

## 実装順序

### Phase 1: 基盤構築

1. **パッケージインストール**
   ```bash
   cd web
   pnpm add langfuse
   ```

2. **ディレクトリ構造作成**
   ```bash
   mkdir -p web/src/lib/llm/{langfuse,prompt}
   ```

3. **型定義作成** (`langfuse/types.ts`)
   - `LangfusePromptMetadata`
   - `PromptVariables`
   - `PromptResult`

4. **インターフェース定義** (`prompt/repository.ts`)
   - `PromptRepository` interface

5. **Langfuseクライアント** (`langfuse/client.ts`)
   - `getLangfuseClient()` 関数
   - シングルトンパターン実装

6. **Repository実装** (`prompt/langfuse-repository.ts`)
   - `LangfusePromptRepository` クラス
   - `getPrompt()` メソッド実装

7. **ファクトリー関数** (`index.ts`)
   - `createPromptRepository()` 関数
   - 型の再エクスポート

### Phase 2: 統合

8. **環境変数設定**
   - `.env` に Langfuse認証情報追加

9. **Langfuse管理画面でプロンプト作成**
   - プロンプト1: `bill-chat-system-normal`（ふつうモード）
   - プロンプト2: `bill-chat-system-hard`（難しいモード）
   - 各プロンプトに変数設定とテンプレート作成

10. **API Route統合** (`app/api/chat/route.ts`)
    - `getDifficultyInstructions()` 関数を削除
    - `createPromptRepository()` 呼び出し
    - 難易度に応じたプロンプト名の決定ロジック追加
    - `experimental_telemetry` 設定追加
    - エラーハンドリング追加

### Phase 3: テスト

11. **動作確認**
    - ローカル環境でLangfuse接続確認
    - プロンプト取得テスト
    - チャット動作確認
    - Langfuseダッシュボードでトレース確認

## 将来的な拡張

### Phase 2: プロンプト管理の拡充

- プロンプトキャッシング機構の追加（同一プロンプトの再取得を削減）
- プロンプトバージョンの明示的指定（現在は最新版を自動取得）
- プロンプトのA/Bテスト機能

### Phase 3: 高度なトレーシング

- `LangfuseSpanProcessor` 導入による詳細なトレーシング
- カスタムスパンの追加（プロンプト取得、変数コンパイルなど）
- マルチモーダルデータの自動検出

### Phase 4: 分析・最適化

- A/Bテスト機能
- プロンプトのパフォーマンス分析
- ユーザーフィードバック連携
- コスト分析とトークン最適化

## 参考資料

- [Langfuse - Vercel AI SDK Integration](https://langfuse.com/docs/integrations/vercel-ai-sdk)
- [Vercel AI SDK - Telemetry](https://ai-sdk.dev/docs/ai-sdk-core/telemetry)
- [Langfuse - Prompt Management for Vercel AI SDK](https://langfuse.com/changelog/2024-11-17-vercel-ai-sdk-prompt-mgmt)
- [Vercel AI SDK - Observability Integrations: Langfuse](https://ai-sdk.dev/providers/observability/langfuse)

## まとめ

この設計では、**最小構成**でLangfuseを導入し、以下を実現します：

✅ プロンプトのバージョン管理（Langfuse管理画面）
✅ OpenTelemetryによる自動トレーシング
✅ 環境別ラベリング（production/preview/development）
✅ シンプルな実装（SpanProcessorなし）
✅ Vercel AI SDKとのシームレスな統合

まずはこの最小構成で動作を確認し、必要に応じてPhase 2以降の機能を追加していく方針です。
